---
updateTime: "2025-2-07 10:30"
desc: "ğŸ’¡GraphRAGè§£è¯»ï¼šè®©LLMå­¦ä¼šç»“æ„åŒ–æ€è€ƒ"
tags: "AI"
outline: deep
---

# å¦‚ä½•å°† DeepSeek-R1 è’¸é¦åˆ°è¾ƒå°æ¨¡å‹ï¼ˆå¦‚ Microsoft çš„ Phi-3-Miniï¼‰

æ·±åº¦å­¦ä¹ æ¨¡å‹å½»åº•æ”¹å˜äº†äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œä½†å…¶åºå¤§çš„è§„æ¨¡å’Œè®¡ç®—éœ€æ±‚å¯èƒ½ä¼šæˆä¸ºå®é™…åº”ç”¨çš„ç“¶é¢ˆã€‚æ¨¡å‹è’¸é¦æ˜¯ä¸€ç§å¼ºå¤§çš„æŠ€æœ¯ï¼Œå®ƒé€šè¿‡å°†çŸ¥è¯†ä»å¤§å‹å¤æ‚æ¨¡å‹ï¼ˆæ•™å¸ˆï¼‰è½¬ç§»åˆ°è¾ƒå°ã€æ›´é«˜æ•ˆçš„æ¨¡å‹ï¼ˆå­¦ç”Ÿï¼‰æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚

åœ¨æœ¬åšå®¢ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»å¦‚ä½•ä½¿ç”¨ LoRAï¼ˆä½ç§©è‡ªé€‚åº”ï¼‰ç­‰ä¸“é—¨æŠ€æœ¯å°† DeepSeek-R1 çš„æ¨ç†èƒ½åŠ›è’¸é¦åˆ°è¾ƒå°çš„æ¨¡å‹ï¼ˆå¦‚ Microsoft çš„ Phi-3-Miniï¼‰ä¸­ã€‚

## 1. ä»€ä¹ˆæ˜¯è’¸é¦ï¼Ÿ

è’¸é¦æ˜¯ä¸€ç§æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œå…¶ä¸­è¾ƒå°çš„æ¨¡å‹ï¼ˆâ€œå­¦ç”Ÿâ€ï¼‰ç»è¿‡è®­ç»ƒä»¥æ¨¡ä»¿è¾ƒå¤§çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆâ€œè€å¸ˆâ€ï¼‰çš„è¡Œä¸ºã€‚ç›®æ ‡æ˜¯ä¿ç•™è€å¸ˆçš„å¤§éƒ¨åˆ†è¡¨ç°ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬å’Œå†…å­˜å ç”¨ã€‚

è¿™ä¸ªæƒ³æ³•æœ€æ—©æ˜¯åœ¨ Geoffrey Hinton å…³äºçŸ¥è¯†è’¸é¦çš„å¼€åˆ›æ€§è®ºæ–‡ä¸­æå‡ºçš„ã€‚å®ƒä¸æ˜¯ç›´æ¥åœ¨åŸå§‹æ•°æ®ä¸Šè®­ç»ƒå­¦ç”Ÿæ¨¡å‹ï¼Œè€Œæ˜¯ä»è€å¸ˆæ¨¡å‹çš„è¾“å‡ºæˆ–ä¸­é—´è¡¨ç¤ºä¸­å­¦ä¹ ã€‚è¿™å®é™…ä¸Šæ˜¯å—åˆ°äººç±»æ•™è‚²çš„å¯å‘ã€‚

### ä¸ºä»€ä¹ˆå®ƒå¾ˆé‡è¦ï¼š
- **æˆæœ¬æ•ˆç‡**ï¼šè¾ƒå°çš„æ¨¡å‹éœ€è¦æ›´å°‘çš„è®¡ç®—èµ„æºã€‚
- **é€Ÿåº¦**ï¼šéå¸¸é€‚åˆå»¶è¿Ÿæ•æ„Ÿçš„åº”ç”¨ç¨‹åºï¼ˆä¾‹å¦‚ APIã€è¾¹ç¼˜è®¾å¤‡ï¼‰ã€‚
- **ä¸“ä¸šåŒ–**ï¼šæ— éœ€é‡æ–°è®­ç»ƒå·¨å‹æ¨¡å‹å³å¯é’ˆå¯¹ç‰¹å®šé¢†åŸŸå®šåˆ¶æ¨¡å‹ã€‚

## 2. è’¸é¦ç±»å‹

æ¨¡å‹è’¸é¦æœ‰å‡ ç§æ–¹æ³•ï¼Œæ¯ç§æ–¹æ³•éƒ½æœ‰å„è‡ªçš„ä¼˜ç‚¹ï¼š

### æ•°æ®è’¸é¦ï¼š
- åœ¨æ•°æ®è’¸é¦ä¸­ï¼Œæ•™å¸ˆæ¨¡å‹ç”Ÿæˆåˆæˆæ•°æ®æˆ–ä¼ªæ ‡ç­¾ï¼Œç„¶åç”¨äºè®­ç»ƒå­¦ç”Ÿæ¨¡å‹ã€‚
- è¿™ç§æ–¹æ³•å¯ä»¥åº”ç”¨äºå¹¿æ³›çš„ä»»åŠ¡ï¼Œå³ä½¿æ˜¯é‚£äº› logits ä¿¡æ¯é‡è¾ƒå°‘çš„ä»»åŠ¡ï¼ˆä¾‹å¦‚å¼€æ”¾å¼æ¨ç†ä»»åŠ¡ï¼‰ã€‚

### Logitsè’¸é¦ï¼š
- Logits æ˜¯åº”ç”¨ softmax å‡½æ•°ä¹‹å‰ç¥ç»ç½‘ç»œçš„åŸå§‹è¾“å‡ºåˆ†æ•°ã€‚
- åœ¨ logitsè’¸é¦ä¸­ï¼Œå­¦ç”Ÿæ¨¡å‹ç»è¿‡è®­ç»ƒä»¥åŒ¹é…æ•™å¸ˆçš„ logitsï¼Œè€Œä¸ä»…ä»…æ˜¯æœ€ç»ˆé¢„æµ‹ã€‚
- è¿™ç§æ–¹æ³•ä¿ç•™äº†æ›´å¤šå…³äºæ•™å¸ˆä¿¡å¿ƒæ°´å¹³å’Œå†³ç­–è¿‡ç¨‹çš„ä¿¡æ¯ã€‚

### ç‰¹å¾è’¸é¦ï¼š
- ç‰¹å¾æç‚¼æ¶‰åŠå°†çŸ¥è¯†ä»æ•™å¸ˆæ¨¡å‹çš„ä¸­é—´å±‚è½¬ç§»åˆ°å­¦ç”Ÿã€‚
- é€šè¿‡å¯¹é½ä¸¤ä¸ªæ¨¡å‹çš„éšè—è¡¨ç¤ºï¼Œå­¦ç”Ÿå¯ä»¥å­¦ä¹ æ›´ä¸°å¯Œã€æ›´æŠ½è±¡çš„ç‰¹å¾ã€‚

## 3. DeepSeek çš„è’¸é¦æ¨¡å‹

ä¸ºäº†ä½¿è®¿é—®æ›´åŠ æ°‘ä¸»åŒ–ï¼ŒDeepSeek AI å‘å¸ƒäº†åŸºäº Qwenï¼ˆQwenï¼Œ2024bï¼‰å’Œ Llamaï¼ˆAI@Metaï¼Œ2024ï¼‰ç­‰æµè¡Œæ¶æ„çš„å…­ä¸ªè’¸é¦å˜ä½“ã€‚ä»–ä»¬ä½¿ç”¨ DeepSeek-R1 ç­–åˆ’çš„ 800k ä¸ªæ ·æœ¬ç›´æ¥å¾®è°ƒå¼€æºæ¨¡å‹ã€‚

å°½ç®¡æ¯” DeepSeek-R1 å°å¾—å¤šï¼Œä½†è’¸é¦æ¨¡å‹åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­éƒ½è¡¨ç°å‡ºè‰²ï¼Œé€šå¸¸å¯ä»¥åŒ¹æ•Œç”šè‡³è¶…è¶Šæ›´å¤§æ¨¡å‹çš„èƒ½åŠ›ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š


Deepseek æç‚¼æ¨¡å‹åŸºå‡†æµ‹è¯• [source](https://arxiv.org/html/2501.12948v1)

## 4. ä¸ºä»€ä¹ˆè¦è’¸é¦è‡ªå·±çš„æ¨¡å‹ï¼Ÿ

### - ç‰¹å®šä»»åŠ¡ä¼˜åŒ–
é¢„è’¸é¦æ¨¡å‹åœ¨å¹¿æ³›çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œçš„åº”ç”¨ç¨‹åºé€šå¸¸éœ€è¦ä¸“ä¸šåŒ–ã€‚

**ç¤ºä¾‹åœºæ™¯**ï¼šä½ æ­£åœ¨æ„å»ºä¸€ä¸ªé‡‘èé¢„æµ‹èŠå¤©æœºå™¨äººã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½¿ç”¨ DeepSeek-R1 ä¸ºé‡‘èæ•°æ®é›†ç”Ÿæˆæ¨ç†è½¨è¿¹ï¼ˆä¾‹å¦‚ï¼Œè‚¡ç¥¨ä»·æ ¼é¢„æµ‹ã€é£é™©åˆ†æï¼‰ï¼Œå¹¶å°†è¿™äº›çŸ¥è¯†è’¸é¦æˆä¸€ä¸ªå·²ç»äº†è§£é‡‘èç»†å¾®å·®åˆ«çš„è¾ƒå°æ¨¡å‹ï¼ˆä¾‹å¦‚ï¼šfinance-LLMï¼‰ã€‚

### - å¤§è§„æ¨¡æˆæœ¬æ•ˆç‡
è™½ç„¶é¢„è’¸é¦æ¨¡å‹æ•ˆç‡å¾ˆé«˜ï¼Œä½†å®ƒä»¬å¯èƒ½ä»ç„¶ä¸é€‚åˆä½ çš„ç‰¹å®šå·¥ä½œé‡ã€‚è’¸é¦ä½ è‡ªå·±çš„æ¨¡å‹å¯ä»¥è®©ä½ é’ˆå¯¹ç¡®åˆ‡çš„èµ„æºé™åˆ¶è¿›è¡Œä¼˜åŒ–ã€‚

### - åŸºå‡†æ€§èƒ½ â‰  çœŸå®ä¸–ç•Œæ€§èƒ½
é¢„è’¸é¦æ¨¡å‹åœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åŸºå‡†æµ‹è¯•é€šå¸¸ä¸èƒ½ä»£è¡¨çœŸå®ä¸–ç•Œçš„ä»»åŠ¡ã€‚å› æ­¤ï¼Œä½ é€šå¸¸éœ€è¦ä¸€ä¸ªåœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸­è¡¨ç°æ¯”ä»»ä½•é¢„è’¸é¦æ¨¡å‹éƒ½æ›´å¥½çš„æ¨¡å‹ã€‚

### - è¿­ä»£æ”¹è¿›
é¢„è’¸é¦æ¨¡å‹æ˜¯é™æ€çš„â€”â€”å®ƒä»¬ä¸ä¼šéšç€æ—¶é—´çš„æ¨ç§»è€Œæ”¹è¿›ã€‚é€šè¿‡è’¸é¦è‡ªå·±çš„æ¨¡å‹ï¼Œä½ å¯ä»¥åœ¨æ–°æ•°æ®å¯ç”¨æ—¶ä¸æ–­å®Œå–„å®ƒã€‚

## 5. å°† DeepSeek-R1 çŸ¥è¯†è’¸é¦æˆè‡ªå®šä¹‰å°æ¨¡å‹

### 5.1 ç”Ÿæˆå’Œæ ¼å¼åŒ–æ•°æ®é›†

ä½ å¯ä»¥é€šè¿‡åœ¨ä½ çš„ç¯å¢ƒä¸­ä½¿ç”¨ ollama æˆ–ä»»ä½•å…¶ä»–éƒ¨ç½²æ¡†æ¶éƒ¨ç½² deepseek-r1 æ¥ç”Ÿæˆè‡ªå®šä¹‰åŸŸç›¸å…³æ•°æ®é›†ã€‚ç„¶è€Œï¼Œå¯¹äºæœ¬æ•™ç¨‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Magpie-Reasoning-V2 æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å« DeepSeek-R1 ç”Ÿæˆçš„ 250K æ€è·¯é“¾ (CoT) æ¨ç†æ ·æœ¬ï¼Œè¿™äº›ç¤ºä¾‹æ¶µç›–äº†æ•°å­¦æ¨ç†ã€ç¼–ç å’Œä¸€èˆ¬é—®é¢˜è§£å†³ç­‰å„ç§ä»»åŠ¡ã€‚

**æ•°æ®é›†ç»“æ„**  
æ¯ä¸ªç¤ºä¾‹åŒ…æ‹¬ï¼š
- **æŒ‡ä»¤**ï¼šä»»åŠ¡æè¿°ï¼ˆä¾‹å¦‚ï¼Œâ€œè§£å†³è¿™ä¸ªæ•°å­¦é—®é¢˜â€ï¼‰ã€‚
- **å“åº”**ï¼šDeepSeek-R1 çš„åˆ†æ­¥æ¨ç† (CoT)ã€‚

ç¤ºä¾‹ï¼š
```json
{
  "instruction": "Solve for x: 2x + 5 = 15",
  "response": "<think>First, subtract 5 from both sides: 2x = 10. Then, divide by 2: x = 5.</think>"
}
```

```python
from datasets import load_dataset

# Load the dataset
dataset = load_dataset("Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B", token="YOUR_HF_TOKEN")
dataset = dataset["train"]

# Format the dataset
def format_instruction(example):
    return {
        "text": (
            "<|user|>\n"
            f"{example['instruction']}\n"
            "<|end|>\n"
            "<|assistant|>\n"
            f"{example['response']}\n"
            "<|end|>"
        )
    }

formatted_dataset = dataset.map(format_instruction, batched=False, remove_columns=subset_dataset.column_names)
formatted_dataset = formatted_dataset.train_test_split(test_size=0.1)  # 90-10 train-test split
```

### 5.2 åŠ è½½æ¨¡å‹å’Œæ ‡è®°å™¨

å‘æ ‡è®°å™¨æ·»åŠ ç‰¹æ®Šæ ‡è®° `<think>` å’Œ `</think>`ã€‚

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = "microsoft/phi-3-mini-4k-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)

# Add custom tokens
CUSTOM_TOKENS = ["<think>", "</think>"]
tokenizer.add_special_tokens({"additional_special_tokens": CUSTOM_TOKENS})
tokenizer.pad_token = tokenizer.eos_token

# Load model with flash attention
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    trust_remote_code=True,
    device_map="auto",
    torch_dtype=torch.float16,
    attn_implementation="flash_attention_2"
)

model.resize_token_embeddings(len(tokenizer))  # Resize for custom tokens
```

### 5.3 é…ç½® LoRA ä»¥å®ç°é«˜æ•ˆå¾®è°ƒ

LoRA é€šè¿‡å†»ç»“åŸºç¡€æ¨¡å‹å¹¶ä»…è®­ç»ƒå°å‹é€‚é…å™¨å±‚æ¥å‡å°‘å†…å­˜ä½¿ç”¨é‡ã€‚

```python
from peft import LoraConfig

peft_config = LoraConfig(
    r=8,  # Rank of the low-rank matrices
    lora_alpha=16,  # Scaling factor
    lora_dropout=0.2,  # Dropout rate
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],  # Target attention layers
    bias="none",  # No bias terms
    task_type="CAUSAL_LM"  # Task type
)
```

### 5.4 è®¾ç½®è®­ç»ƒå‚æ•°

```python
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./phi-3-deepseek-finetuned",
    num_train_epochs=3,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=4,
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_strategy="steps",
    logging_steps=50,
    learning_rate=2e-5,
    fp16=True,
    optim="paged_adamw_32bit",
    max_grad_norm=0.3,
    warmup_ratio=0.03,
    lr_scheduler_type="cosine"
)
```

### 5.5 è®­ç»ƒæ¨¡å‹

```python
from trl import SFTTrainer
from transformers import DataCollatorForLanguageModeling

# Data collator
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# Trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=formatted_dataset["train"],
    eval_dataset=formatted_dataset["test"],
    data_collator=data_collator,
    peft_config=peft_config
)

# Start training
trainer.train()
trainer.save_model("./phi-3-deepseek-finetuned")
tokenizer.save_pretrained("./phi-3-deepseek-finetuned")
```

### 5.6 åˆå¹¶ä¿å­˜æœ€ç»ˆæ¨¡å‹

```python
final_model = trainer.model.merge_and_unload()
final_model.save_pretrained("./phi-3-deepseek-finetuned-final")
tokenizer.save_pretrained("./phi-3-deepseek-finetuned-final")
```

### 5.7 æ¨ç†

```python
from transformers import pipeline

# Load fine-tuned model
model = AutoModelForCausalLM.from_pretrained(
    "./phi-3-deepseek-finetuned-final",
    device_map="auto",
    torch_dtype=torch.float16
)
tokenizer = AutoTokenizer.from_pretrained("./phi-3-deepseek-finetuned-final")
model.resize_token_embeddings(len(tokenizer))

# Create chat pipeline
chat_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device_map="auto"
)

# Generate response
prompt = """<|user|>
What's the probability of rolling a 7 with two dice?
<|end|>
<|assistant|>
"""
output = chat_pipeline(
    prompt,
    max_new_tokens=5000,
    temperature=0.7,
    do_sample=True,
    eos_token_id=tokenizer.eos_token_id
)
print(output[0]['generated_text'])
```

## 6. è’¸é¦å‰åçš„æ¨ç†

- **è’¸é¦å‰çš„æ¨ç†**  
å“åº”ç®€å•æ˜äº†ã€‚å®ƒç›´æ¥æä¾›äº†è®¡ç®—ç­”æ¡ˆçš„æ­¥éª¤ã€‚


- **è’¸é¦åçš„æ¨ç†**  
è’¸é¦åçš„å“åº”å¼•å…¥äº†ä¸€ç§æ›´è¯¦ç»†å’Œç»“æ„åŒ–çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸€ä¸ªæ˜ç¡®çš„â€œæ€è€ƒâ€éƒ¨åˆ†ï¼Œæ¦‚è¿°äº†æ€ç»´è¿‡ç¨‹å’Œæ¨ç†ï¼Œè¿™å¯¹äºä¸ºå¤æ‚é—®é¢˜ç”Ÿæˆå‡†ç¡®çš„å“åº”éå¸¸æœ‰å¸®åŠ©ã€‚



æœ€ç»ˆï¼Œå°†è’¸é¦åçš„æ¨¡å‹æƒé‡æ¨é€åˆ° huggingface hubï¼ˆrepo_idï¼šGPD1/DeepSeek-R1-Distill-phi-3-mini-4k-lorar8-alpha16â€“50000samplesï¼‰ã€‚

åŸæ–‡é“¾æ¥ï¼š[How to distill Deepseek-R1: A Comprehensive Guide](https://medium.com/@prabhudev.guntur/how-to-distill-deepseek-r1-a-comprehensive-guide-c8ba04e2c28c)

Feel free to explore the model, benchmark it against your tasks, and share your feedback! ğŸš€